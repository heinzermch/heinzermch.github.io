---
layout: post
author: Michael Heinzer
title:  "On RLHF, DPO And PPO"
description: A quick overview on finetuning LLMs beyond classification and regression.
date:   2025-01-02 18:00:00 +0530
categories: LLMs SFT RLHF DPO PPO ReinforcementLearning
comments: yes
published: true
---
Intro

Want to understand supervised fine tuning (SFT) beyond classification and regression problems. Not yet touching on the most recent topics of RL reasoning that used for OpenAIs o1 type of model.

This is reinforcement learning, gradient not directly available from a loss function like cross-entropy or MSE.


## Basic Concepts and Notation

Before we start, let us quickly repeat some basic concepts and their notation. Readers familiar with the topic may skip this section. This is not meant to be an introduction to probability theory or other mathematical concepts, only a quick refresh of what we will need later on.

- **Reinforcement Learning**: In reinforcement learning an agent is taking actions $$a_t$$ in an environment in state $$s_t$$ and getting rewards $$r_t$$.

![Reinforcement Learning Overview](/assets/images/rlhf_ppo_dpo/rl_diagram_transparent_bg.png)


- **Policy**: A rule used by an agent to decide what actions to take, if it is stochastic it is denoted by $$\pi $$. It samples actions $$a_t$$ from states $$s_t $$: $$ a_t \sim \pi( \cdot \mid s_t)$$.

- **Parametrized Policy**: In deep RL we deal with parametrized policies, where the parameters are denoted by $$\theta$$. Hence we write $$ a_t \sim \pi_{\theta}(\cdot \mid s_t).$$.

- **On-Policy Value Function**:  $$V^{\pi}(s)$$, which gives the expected return if you start in state $$s$$ and always act according to policy $$\pi$$:

$$ V^{\pi}(s) = E_{\tau \sim \pi} \big(R(\tau) \mid s_0 = s \big)$$

- **On-Policy Action-Value Function**:  $$Q^{\pi}(s,a)$$, which gives the expected return if you start in state $$s$$, take an arbitrary action $$a$$ (not necessarily from the policy $$\pi$$), and then afterwards act according to policy $$\pi$$:

 $$Q^{\pi}(s,a) = E_{\tau \sim \pi} \big(R(\tau) \mid s_0 = s, a_0 = a \big)$$

- **Advantage Function**: The advantage function $$A^{\pi}(s,a)$$ corresponding to a policy $$\pi$$ describes how much better it is to take a specific action $$a$$ in state $$s$$, over randomly selecting an action according to $$\pi(\cdot \mid s)$$. The advantage function is defined by:

$$A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s)$$


- **Kullback-Leibler Divergence**: The KL-divergence of two probability distributions $$p$$ and $$q$$ is defined as

$$D_{KL}(p \mid\mid q) := \sum_{x \in X} p(x) \log\bigg(\frac{p(x)}{q(x)}\bigg) = - \sum_{x \in X} p(x) \log\bigg(\frac{q(x)}{p(x)}\bigg).$$

Often we consider $$p$$ to be the true distribution and $$q$$ the approximation or model output. Then the KL-divergence would give us a measure of how much information is lost when we approximate $$p$$ with $$q$$.






# Reinforcement Learning from Human Feedback

RLHF: Use reinforcement learning to train a reward model, nowadays typically a LLM, on human feedback. Here human feedback here is a preference rating between two answers, i.e. is answer one or answer two better. We do this because it is hard to quantify and distill what makes one answer better than another.

Use the trained reward model to rate new and many more answers generated by a baseline LLM without using humans. The feedback from the reward model is then used to train the baseline LLM. This is also sometimes called alignment.

How do we update the baseline model? The signal we are getting is still only the preference between two answers, we do not assign a single score to to each answer? (Do we?). We use a reward signal to update the baseline model, rewards are typically what is used in Reinforcement leanring methos. The method often used in the RLHF framework is Proximal Policy Optimization (PPO).

![An overview of the three concepts](/assets/images/rlhf_ppo_dpo/overview.png)


PPO is used to finetune the baseline LLM based on rewards by the reward model. PPO is designed to be more stable and efficient than traditional policy gradient methods.

DPO is an alternative to use PPO within RLHF. Simplify the fine-tuning process by directly optimizing the LLM based on the human ratings, without training the reward model. Reformulate the RL problem into a simpler classification problem, simplifying the answer.

RLHF is for cases where there is often no single right answer, we only give vague directions towards a better formulated response.

Note: Why train a reward model first and then use the imperfect model to train and LLM? Do we lose some information in that process that could be directly propagated to the LLM? Or do we gain more by having more samples? Assuming the reward model is much more capable (larger) than the LLM we train, does it make sense to use it then? Reminds me of synthetic data generation or pseudo labeling, distillation process.


# Proximal Policy Optimization

An overview of the 2017 paper from OpenAI called Proximal Policy Optimization Algorithms. This is pre-LLM craze, so the context for this paper is playing games, and that is what is evaluated on.


## Background

### Policy Gradient Methods

We compute an estimator of the policy gradient and plug it into a stochastic gradient ascent algorithm. A commonly used gradient estimator is

$$ \hat{g} = \hat{E_t} \bigg( \nabla_{\theta} \log(\pi_{\theta} (a_t \mid s_t) \hat{A_t} \bigg)$$

The hats over $$g$$ and $$E$$ denote that we empirically estimate the quantities over a batch of samples. Typically alternating between sampling and optimization. In this case we gradient extimate $$\hat{g}$$ is obtained by differentiating the loss function:

$$L^{PG}(\theta) = \hat{E_t} \bigg( \log \big(\pi_{\theta}(a_t \mid s_t) \big) \hat{A_t} \bigg)$$

### Trust Region Methods
Unconstrained optimization has been problematic for large updates and can derail training. Trust region methods try to mitigate this by only updating within a small trusted region where we believe that the approximation of the objective function is reasonably accurate. How do we define the trusted region? By ensuring that the KL-Divergence between the original and the updated policy stays small.

A typical method is Trust Region Policy Optimization (TRPO) which


$$\begin{align*}
\text{maximize} \: & \hat{E_t} \bigg[ \frac{\pi_{\theta}(a_t \mid s_t)}{\pi_{\theta_{old}}(a_t \mid s_t)} \hat{A_t} \bigg] \\ 
\text{subject to} \: & \hat{E_t} [ KL[\pi_{\theta}(a_t \mid s_t) \mid \pi_{\theta_{old}}(a_t \mid s_t)]] \leq \delta
\end{align*}$$


The theory behing TRPO suggests using a KL penalty and optimizing 

$$\hat{E_t} \bigg[ \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)} \hat{A_t} - \beta KL[\pi_{\theta}(a_t | s_t) | \pi_{\theta_{old}}(a_t | s_t)]] \bigg]  $$

instead. The choice of $$\beta$$ is tricky and changes with every task.


### Clipped Surrogate Objective

Let $$r_t(\theta)$$ denote the probability ratio $$r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}$$. It follows that $$r(\thetha_{old})$$ = 1.

TRPO maximazes a "surrogate" objective 

$$L^{CPI}(\theta) =\hat{E_t} \bigg[\frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)} \hat{A_t} \bigg] = \hat{E_t} \big[r_t(\theta) \hat{A_t}\big]$$

where CPI refers to Conservative Policy Iteration. Again this would have issues for large policy updates, which can be mitigated by clipping values of $$r_t(\theta) \hat{A_t} $$ that move too far away from 1.

$$ L^{CLIP}(\theta) = \hat{E_t} \bigg[ min(r_t(\theta) \hat{A_t}, \text{clip}(r_t(\theta), 1- \epsilon, 1+\epsilon) \hat{A_t} \bigg]$$

where $$\epsilon > 0$$ is a hyperparameter. Empirically $$\epsilon = 0.2$$ has been found to work well. What is the motivation behind this change?


## Other

Optimize a surrogate objective function using SGA (ascent instead of descent. Instead of one gradient update per sample, do minibatch epoch updates. Previous work Trust Region Policy Optimization (TRPO).

Using NN that shares parameters between policy and value function.

Use loss function that combines policy surrogate and value function error term

PPO uses only first order optimization

Note: Paper was written in a more general form, for length T trajectory segments. In case of LLMs we would only use T=1. We generate an answer and get a reward immediately.

What does Actor Critic style mean? Actor is LLM and Critic the reward model.

Can I add some code here to show an implementation?

Concepts: Stochastic Gradient Ascent, Policy Gradient Methods, TRPO, conjugate gradient algorithm, variance-reduced advantage function estimators, learned state-value function V(s).

# Direct Preference Optimization


Replace PPO based methods with DPO, re-phrase into a simple classification problem. 

RLHF maximizes an estimated reward without drifting too far from original model.

DPO enables extraction of optimal policy in closed from, solve RLHF with a simple classification loss.

(Quick section why we want RLHF for LLMs, bias model towards rare high quality output, not necessary median opinions or ability)

Bradley-Terry model?? Placket-Luce ranking models

Single stage policy learning approach.



Note: This still seems bounded by best human performance on a task, even if it can do many tasks. Getting the required data is another headache.









# Conclusion

TODO.

## References

A list of resources used to write this post, also useful for further reading:

- [Proximal Policy Optimization Algorithms - John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov](https://arxiv.org/abs/1707.06347) for the original DPO paper by OpenAI.
 
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model - Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn](https://arxiv.org/abs/2305.18290) for the DPO paper by Stanford.
- [Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback - Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi](https://arxiv.org/abs/2406.09279) for a paper that compares DPO and PPO on different tasks.
- [Preference fine-tuning API by OpenAI](https://platform.openai.com/docs/guides/fine-tuning#preference) for an example of an FT API.
- [Introduction to RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) by OpenAI Spinning UP.


## Comments

I would be happy to hear about any mistakes or inconsistencies in my post. Other suggestions are of course also welcome. You can either write a comment below or find my email address on the [about page](https://heinzermch.github.io/about/).
