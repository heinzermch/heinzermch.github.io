---
layout: post
author: Michael Heinzer
title:  "On Transformers and Attention"
description: Taking notes and summarizing
date:   2021-04-28 11:00:00 +0530
categories: DeepLearning NLP Transformer Attention Self-Attention ComputerVision Convolution FullyConnected
comments: no
published: no
---
In the past couple of years, many fields have been revolutionized by the use of transformer based architectures. A crucial part of that architecture is the self-attention layer. In this post we will have a brief look at what makes the self-attention layer different from previously known layers such as the convolutional or the fully connected one.

## Basic Concepts and Notation

Before we start, let us quickly repeat some basic concepts and their notation. Readers familiar with the topic may skip this section. This is not meant to be an introduction to probability theory or other mathematical concepts, only a quick refresh of what we will need later on.

- **Definition**: If a term on the left hand side of the equation is defined as as the right hand side term, then $$:=$$ will be used. This is similar to setting a variable in programming. As an example we can set $$g(x)$$ to be $$x^2$$ by writing $$g(x) := x^2$$. In mathematics, when writing simply $$=$$ means that the left side implies the right (denoted by $$\Rightarrow$$) and right side the left (denoted by $$\Leftarrow$$) at the same time.
- **Group**:
- **Operator**:
- **Equivariance**:
- **Invariance**:

# A review of layers

## Fully connected

## Convolutional

## Self-Attention

# Consequences

## Transformer Architecture

## Applications

# References

- Definition of a group: https://en.wikipedia.org/wiki/Group_(mathematics)
- Symmetry Group: https://en.wikipedia.org/wiki/Symmetry_group

# Comments